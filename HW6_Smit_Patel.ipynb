{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHqGSvOTFO+Z+P9UTmuFHq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smitpatel201/IntroToMachineLearning/blob/main/HW6_Smit_Patel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "ZjQSm016B_2y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access the dataset\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/Housing.csv'\n",
        "\n",
        "# Load the dataset into a DataFrame\n",
        "housing_data = pd.read_csv(file_path)\n",
        "\n",
        "# Fill any NaN values to ensure data consistency\n",
        "housing_data.fillna(method='ffill', inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-KUnIW4B-8q",
        "outputId": "b30182e7-c672-4419-d9a9-c30b14d69832"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 1a\n",
        "\n",
        "# Separate the features and the target variable\n",
        "features = housing_data.drop('price', axis=1)\n",
        "target = housing_data['price']\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_columns = features.select_dtypes(include=['object']).columns\n",
        "numerical_columns = features.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Define preprocessor with scaling for numerical and encoding for categorical data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_columns),\n",
        "        ('cat', OneHotEncoder(), categorical_columns)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply preprocessing to the features\n",
        "X_processed = preprocessor.fit_transform(features)\n",
        "\n",
        "# Convert the processed features and target to PyTorch tensors\n",
        "X_tensor = torch.tensor(X_processed.astype(np.float32))\n",
        "y_tensor = torch.tensor(target.values.astype(np.float32)).unsqueeze(1)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a simple neural network model with one hidden layer using Tanh activation\n",
        "input_size = X_train.shape[1]\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(input_size, 32),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(32, 1)  # Removed Sigmoid activation\n",
        ")\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "epochs = 500\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    y_train_pred = model(X_train)\n",
        "    train_loss = criterion(y_train_pred, y_train)\n",
        "    train_losses.append(train_loss.item())\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights\n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validation loss\n",
        "    with torch.no_grad():\n",
        "        y_val_pred = model(X_val)\n",
        "        val_loss = criterion(y_val_pred, y_val)\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "    # Print the training and validation loss with the epoch number\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Training Loss: {train_loss:.4f} - Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "# Plotting training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss - 1 hidden layer')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-T8DPzphDOW5",
        "outputId": "6a4a6dda-db3d-4ed0-cd55-be887d53c1c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500 - Training Loss: 25234788843520.0000 - Validation Loss: 30034156847104.0000\n",
            "Epoch 2/500 - Training Loss: 25144722456576.0000 - Validation Loss: 27031536402432.0000\n",
            "Epoch 3/500 - Training Loss: 22328727568384.0000 - Validation Loss: 24400673898496.0000\n",
            "Epoch 4/500 - Training Loss: 19872174047232.0000 - Validation Loss: 22094888828928.0000\n",
            "Epoch 5/500 - Training Loss: 17729186693120.0000 - Validation Loss: 20073379004416.0000\n",
            "Epoch 6/500 - Training Loss: 15859738214400.0000 - Validation Loss: 18300532686848.0000\n",
            "Epoch 7/500 - Training Loss: 14228914176000.0000 - Validation Loss: 16745222897664.0000\n",
            "Epoch 8/500 - Training Loss: 12806252396544.0000 - Validation Loss: 15380264255488.0000\n",
            "Epoch 9/500 - Training Loss: 11565189300224.0000 - Validation Loss: 14181898125312.0000\n",
            "Epoch 10/500 - Training Loss: 10482539823104.0000 - Validation Loss: 13129360605184.0000\n",
            "Epoch 11/500 - Training Loss: 9538083225600.0000 - Validation Loss: 12204511330304.0000\n",
            "Epoch 12/500 - Training Loss: 8714181410816.0000 - Validation Loss: 11391490588672.0000\n",
            "Epoch 13/500 - Training Loss: 7995442331648.0000 - Validation Loss: 10676438302720.0000\n",
            "Epoch 14/500 - Training Loss: 7368451031040.0000 - Validation Loss: 10047223496704.0000\n",
            "Epoch 15/500 - Training Loss: 6821487575040.0000 - Validation Loss: 9493257650176.0000\n",
            "Epoch 16/500 - Training Loss: 6344342503424.0000 - Validation Loss: 9005266108416.0000\n",
            "Epoch 17/500 - Training Loss: 5928101347328.0000 - Validation Loss: 8575141806080.0000\n",
            "Epoch 18/500 - Training Loss: 5564991537152.0000 - Validation Loss: 8195789553664.0000\n",
            "Epoch 19/500 - Training Loss: 5248231407616.0000 - Validation Loss: 7860998111232.0000\n",
            "Epoch 20/500 - Training Loss: 4971901747200.0000 - Validation Loss: 7565340049408.0000\n",
            "Epoch 21/500 - Training Loss: 4730845659136.0000 - Validation Loss: 7304053260288.0000\n",
            "Epoch 22/500 - Training Loss: 4520557936640.0000 - Validation Loss: 7072974897152.0000\n",
            "Epoch 23/500 - Training Loss: 4337112711168.0000 - Validation Loss: 6868455391232.0000\n",
            "Epoch 24/500 - Training Loss: 4177082187776.0000 - Validation Loss: 6687300255744.0000\n",
            "Epoch 25/500 - Training Loss: 4037479759872.0000 - Validation Loss: 6526706647040.0000\n",
            "Epoch 26/500 - Training Loss: 3915696046080.0000 - Validation Loss: 6384218800128.0000\n",
            "Epoch 27/500 - Training Loss: 3809457471488.0000 - Validation Loss: 6257685037056.0000\n",
            "Epoch 28/500 - Training Loss: 3716780130304.0000 - Validation Loss: 6145214775296.0000\n",
            "Epoch 29/500 - Training Loss: 3635931774976.0000 - Validation Loss: 6045151789056.0000\n",
            "Epoch 30/500 - Training Loss: 3565403766784.0000 - Validation Loss: 5956039606272.0000\n",
            "Epoch 31/500 - Training Loss: 3503877783552.0000 - Validation Loss: 5876602634240.0000\n",
            "Epoch 32/500 - Training Loss: 3450205110272.0000 - Validation Loss: 5805716275200.0000\n",
            "Epoch 33/500 - Training Loss: 3403384094720.0000 - Validation Loss: 5742395916288.0000\n",
            "Epoch 34/500 - Training Loss: 3362538913792.0000 - Validation Loss: 5685771239424.0000\n",
            "Epoch 35/500 - Training Loss: 3326907514880.0000 - Validation Loss: 5635080978432.0000\n",
            "Epoch 36/500 - Training Loss: 3295824576512.0000 - Validation Loss: 5589651947520.0000\n",
            "Epoch 37/500 - Training Loss: 3268708401152.0000 - Validation Loss: 5548892749824.0000\n",
            "Epoch 38/500 - Training Loss: 3245054099456.0000 - Validation Loss: 5512282767360.0000\n",
            "Epoch 39/500 - Training Loss: 3224418648064.0000 - Validation Loss: 5479360102400.0000\n",
            "Epoch 40/500 - Training Loss: 3206417743872.0000 - Validation Loss: 5449720528896.0000\n",
            "Epoch 41/500 - Training Loss: 3190714531840.0000 - Validation Loss: 5423004909568.0000\n",
            "Epoch 42/500 - Training Loss: 3177015410688.0000 - Validation Loss: 5398897623040.0000\n",
            "Epoch 43/500 - Training Loss: 3165065314304.0000 - Validation Loss: 5377117650944.0000\n",
            "Epoch 44/500 - Training Loss: 3154640371712.0000 - Validation Loss: 5357417005056.0000\n",
            "Epoch 45/500 - Training Loss: 3145545809920.0000 - Validation Loss: 5339577581568.0000\n",
            "Epoch 46/500 - Training Loss: 3137612808192.0000 - Validation Loss: 5323404345344.0000\n",
            "Epoch 47/500 - Training Loss: 3130691682304.0000 - Validation Loss: 5308725329920.0000\n",
            "Epoch 48/500 - Training Loss: 3124653981696.0000 - Validation Loss: 5295388491776.0000\n",
            "Epoch 49/500 - Training Loss: 3119387246592.0000 - Validation Loss: 5283255418880.0000\n",
            "Epoch 50/500 - Training Loss: 3114792648704.0000 - Validation Loss: 5272206573568.0000\n",
            "Epoch 51/500 - Training Loss: 3110784729088.0000 - Validation Loss: 5262133428224.0000\n",
            "Epoch 52/500 - Training Loss: 3107288252416.0000 - Validation Loss: 5252942135296.0000\n",
            "Epoch 53/500 - Training Loss: 3104237682688.0000 - Validation Loss: 5244544090112.0000\n",
            "Epoch 54/500 - Training Loss: 3101577445376.0000 - Validation Loss: 5236865368064.0000\n",
            "Epoch 55/500 - Training Loss: 3099256160256.0000 - Validation Loss: 5229835190272.0000\n",
            "Epoch 56/500 - Training Loss: 3097230835712.0000 - Validation Loss: 5223394836480.0000\n",
            "Epoch 57/500 - Training Loss: 3095464509440.0000 - Validation Loss: 5217488207872.0000\n",
            "Epoch 58/500 - Training Loss: 3093923364864.0000 - Validation Loss: 5212066545664.0000\n",
            "Epoch 59/500 - Training Loss: 3092579352576.0000 - Validation Loss: 5207085285376.0000\n",
            "Epoch 60/500 - Training Loss: 3091406520320.0000 - Validation Loss: 5202505105408.0000\n",
            "Epoch 61/500 - Training Loss: 3090383896576.0000 - Validation Loss: 5198290354176.0000\n",
            "Epoch 62/500 - Training Loss: 3089491034112.0000 - Validation Loss: 5194409050112.0000\n",
            "Epoch 63/500 - Training Loss: 3088712466432.0000 - Validation Loss: 5190831308800.0000\n",
            "Epoch 64/500 - Training Loss: 3088033513472.0000 - Validation Loss: 5187532488704.0000\n",
            "Epoch 65/500 - Training Loss: 3087441068032.0000 - Validation Loss: 5184487424000.0000\n",
            "Epoch 66/500 - Training Loss: 3086924382208.0000 - Validation Loss: 5181674618880.0000\n",
            "Epoch 67/500 - Training Loss: 3086472970240.0000 - Validation Loss: 5179076771840.0000\n",
            "Epoch 68/500 - Training Loss: 3086080540672.0000 - Validation Loss: 5176672387072.0000\n",
            "Epoch 69/500 - Training Loss: 3085736869888.0000 - Validation Loss: 5174449930240.0000\n",
            "Epoch 70/500 - Training Loss: 3085437763584.0000 - Validation Loss: 5172391575552.0000\n",
            "Epoch 71/500 - Training Loss: 3085176668160.0000 - Validation Loss: 5170486312960.0000\n",
            "Epoch 72/500 - Training Loss: 3084949127168.0000 - Validation Loss: 5168718938112.0000\n",
            "Epoch 73/500 - Training Loss: 3084750422016.0000 - Validation Loss: 5167081586688.0000\n",
            "Epoch 74/500 - Training Loss: 3084576620544.0000 - Validation Loss: 5165563248640.0000\n",
            "Epoch 75/500 - Training Loss: 3084425625600.0000 - Validation Loss: 5164153438208.0000\n",
            "Epoch 76/500 - Training Loss: 3084293767168.0000 - Validation Loss: 5162846388224.0000\n",
            "Epoch 77/500 - Training Loss: 3084178948096.0000 - Validation Loss: 5161631612928.0000\n",
            "Epoch 78/500 - Training Loss: 3084078284800.0000 - Validation Loss: 5160502820864.0000\n",
            "Epoch 79/500 - Training Loss: 3083990990848.0000 - Validation Loss: 5159454244864.0000\n",
            "Epoch 80/500 - Training Loss: 3083914706944.0000 - Validation Loss: 5158480117760.0000\n",
            "Epoch 81/500 - Training Loss: 3083848122368.0000 - Validation Loss: 5157573623808.0000\n",
            "Epoch 82/500 - Training Loss: 3083789664256.0000 - Validation Loss: 5156731617280.0000\n",
            "Epoch 83/500 - Training Loss: 3083739070464.0000 - Validation Loss: 5155946758144.0000\n",
            "Epoch 84/500 - Training Loss: 3083694768128.0000 - Validation Loss: 5155217997824.0000\n",
            "Epoch 85/500 - Training Loss: 3083656232960.0000 - Validation Loss: 5154538520576.0000\n",
            "Epoch 86/500 - Training Loss: 3083622416384.0000 - Validation Loss: 5153906229248.0000\n",
            "Epoch 87/500 - Training Loss: 3083593318400.0000 - Validation Loss: 5153317453824.0000\n",
            "Epoch 88/500 - Training Loss: 3083567366144.0000 - Validation Loss: 5152770097152.0000\n",
            "Epoch 89/500 - Training Loss: 3083545083904.0000 - Validation Loss: 5152258916352.0000\n",
            "Epoch 90/500 - Training Loss: 3083525423104.0000 - Validation Loss: 5151783911424.0000\n",
            "Epoch 91/500 - Training Loss: 3083508645888.0000 - Validation Loss: 5151339839488.0000\n",
            "Epoch 92/500 - Training Loss: 3083493703680.0000 - Validation Loss: 5150927224832.0000\n",
            "Epoch 93/500 - Training Loss: 3083481120768.0000 - Validation Loss: 5150542397440.0000\n",
            "Epoch 94/500 - Training Loss: 3083469324288.0000 - Validation Loss: 5150183784448.0000\n",
            "Epoch 95/500 - Training Loss: 3083459624960.0000 - Validation Loss: 5149849812992.0000\n",
            "Epoch 96/500 - Training Loss: 3083450974208.0000 - Validation Loss: 5149536288768.0000\n",
            "Epoch 97/500 - Training Loss: 3083443634176.0000 - Validation Loss: 5149246357504.0000\n",
            "Epoch 98/500 - Training Loss: 3083437080576.0000 - Validation Loss: 5148974776320.0000\n",
            "Epoch 99/500 - Training Loss: 3083431313408.0000 - Validation Loss: 5148721545216.0000\n",
            "Epoch 100/500 - Training Loss: 3083426332672.0000 - Validation Loss: 5148486139904.0000\n",
            "Epoch 101/500 - Training Loss: 3083422138368.0000 - Validation Loss: 5148265938944.0000\n",
            "Epoch 102/500 - Training Loss: 3083418468352.0000 - Validation Loss: 5148059893760.0000\n",
            "Epoch 103/500 - Training Loss: 3083415060480.0000 - Validation Loss: 5147868528640.0000\n",
            "Epoch 104/500 - Training Loss: 3083412176896.0000 - Validation Loss: 5147690795008.0000\n",
            "Epoch 105/500 - Training Loss: 3083409293312.0000 - Validation Loss: 5147523022848.0000\n",
            "Epoch 106/500 - Training Loss: 3083407458304.0000 - Validation Loss: 5147367309312.0000\n",
            "Epoch 107/500 - Training Loss: 3083405361152.0000 - Validation Loss: 5147221032960.0000\n",
            "Epoch 108/500 - Training Loss: 3083403526144.0000 - Validation Loss: 5147085766656.0000\n",
            "Epoch 109/500 - Training Loss: 3083402215424.0000 - Validation Loss: 5146959413248.0000\n",
            "Epoch 110/500 - Training Loss: 3083401166848.0000 - Validation Loss: 5146840924160.0000\n",
            "Epoch 111/500 - Training Loss: 3083399856128.0000 - Validation Loss: 5146730823680.0000\n",
            "Epoch 112/500 - Training Loss: 3083398545408.0000 - Validation Loss: 5146628063232.0000\n",
            "Epoch 113/500 - Training Loss: 3083398021120.0000 - Validation Loss: 5146531594240.0000\n",
            "Epoch 114/500 - Training Loss: 3083396972544.0000 - Validation Loss: 5146442465280.0000\n",
            "Epoch 115/500 - Training Loss: 3083396710400.0000 - Validation Loss: 5146358579200.0000\n",
            "Epoch 116/500 - Training Loss: 3083395923968.0000 - Validation Loss: 5146280460288.0000\n",
            "Epoch 117/500 - Training Loss: 3083395661824.0000 - Validation Loss: 5146207584256.0000\n",
            "Epoch 118/500 - Training Loss: 3083395661824.0000 - Validation Loss: 5146138902528.0000\n",
            "Epoch 119/500 - Training Loss: 3083394875392.0000 - Validation Loss: 5146075987968.0000\n",
            "Epoch 120/500 - Training Loss: 3083394613248.0000 - Validation Loss: 5146016219136.0000\n",
            "Epoch 121/500 - Training Loss: 3083394088960.0000 - Validation Loss: 5145960120320.0000\n",
            "Epoch 122/500 - Training Loss: 3083394088960.0000 - Validation Loss: 5145908740096.0000\n",
            "Epoch 123/500 - Training Loss: 3083394088960.0000 - Validation Loss: 5145860505600.0000\n",
            "Epoch 124/500 - Training Loss: 3083393302528.0000 - Validation Loss: 5145815416832.0000\n",
            "Epoch 125/500 - Training Loss: 3083393826816.0000 - Validation Loss: 5145772949504.0000\n",
            "Epoch 126/500 - Training Loss: 3083393302528.0000 - Validation Loss: 5145733103616.0000\n",
            "Epoch 127/500 - Training Loss: 3083393040384.0000 - Validation Loss: 5145696403456.0000\n",
            "Epoch 128/500 - Training Loss: 3083393040384.0000 - Validation Loss: 5145662849024.0000\n",
            "Epoch 129/500 - Training Loss: 3083392778240.0000 - Validation Loss: 5145630343168.0000\n",
            "Epoch 130/500 - Training Loss: 3083393040384.0000 - Validation Loss: 5145599934464.0000\n",
            "Epoch 131/500 - Training Loss: 3083392778240.0000 - Validation Loss: 5145572671488.0000\n",
            "Epoch 132/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145546457088.0000\n",
            "Epoch 133/500 - Training Loss: 3083392778240.0000 - Validation Loss: 5145521815552.0000\n",
            "Epoch 134/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145499271168.0000\n",
            "Epoch 135/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145477775360.0000\n",
            "Epoch 136/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145457852416.0000\n",
            "Epoch 137/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145439502336.0000\n",
            "Epoch 138/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145422200832.0000\n",
            "Epoch 139/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145406472192.0000\n",
            "Epoch 140/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145391267840.0000\n",
            "Epoch 141/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145377112064.0000\n",
            "Epoch 142/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145363480576.0000\n",
            "Epoch 143/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145351421952.0000\n",
            "Epoch 144/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145339887616.0000\n",
            "Epoch 145/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145329401856.0000\n",
            "Epoch 146/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145318391808.0000\n",
            "Epoch 147/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145310003200.0000\n",
            "Epoch 148/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145300566016.0000\n",
            "Epoch 149/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145292701696.0000\n",
            "Epoch 150/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145284837376.0000\n",
            "Epoch 151/500 - Training Loss: 3083392778240.0000 - Validation Loss: 5145277497344.0000\n",
            "Epoch 152/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145271730176.0000\n",
            "Epoch 153/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145264914432.0000\n",
            "Epoch 154/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145259147264.0000\n",
            "Epoch 155/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145253380096.0000\n",
            "Epoch 156/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145248661504.0000\n",
            "Epoch 157/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145243942912.0000\n",
            "Epoch 158/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145239748608.0000\n",
            "Epoch 159/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145235554304.0000\n",
            "Epoch 160/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145232408576.0000\n",
            "Epoch 161/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145228214272.0000\n",
            "Epoch 162/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145225068544.0000\n",
            "Epoch 163/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145221922816.0000\n",
            "Epoch 164/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145218252800.0000\n",
            "Epoch 165/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145215107072.0000\n",
            "Epoch 166/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145213009920.0000\n",
            "Epoch 167/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145210388480.0000\n",
            "Epoch 168/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145208291328.0000\n",
            "Epoch 169/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145207242752.0000\n",
            "Epoch 170/500 - Training Loss: 3083392778240.0000 - Validation Loss: 5145204621312.0000\n",
            "Epoch 171/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145202524160.0000\n",
            "Epoch 172/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145200951296.0000\n",
            "Epoch 173/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145199902720.0000\n",
            "Epoch 174/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145197281280.0000\n",
            "Epoch 175/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145196232704.0000\n",
            "Epoch 176/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145195708416.0000\n",
            "Epoch 177/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145194135552.0000\n",
            "Epoch 178/500 - Training Loss: 3083392778240.0000 - Validation Loss: 5145192562688.0000\n",
            "Epoch 179/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145191514112.0000\n",
            "Epoch 180/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145190989824.0000\n",
            "Epoch 181/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145189941248.0000\n",
            "Epoch 182/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145189416960.0000\n",
            "Epoch 183/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145187844096.0000\n",
            "Epoch 184/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145187319808.0000\n",
            "Epoch 185/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145186795520.0000\n",
            "Epoch 186/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145186795520.0000\n",
            "Epoch 187/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145185746944.0000\n",
            "Epoch 188/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145184698368.0000\n",
            "Epoch 189/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145184698368.0000\n",
            "Epoch 190/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145184174080.0000\n",
            "Epoch 191/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145183649792.0000\n",
            "Epoch 192/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145183125504.0000\n",
            "Epoch 193/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145182601216.0000\n",
            "Epoch 194/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145182601216.0000\n",
            "Epoch 195/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145182601216.0000\n",
            "Epoch 196/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145181552640.0000\n",
            "Epoch 197/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 198/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181552640.0000\n",
            "Epoch 199/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 200/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145179979776.0000\n",
            "Epoch 201/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179979776.0000\n",
            "Epoch 202/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145179979776.0000\n",
            "Epoch 203/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179979776.0000\n",
            "Epoch 204/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 205/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 206/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 207/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 208/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 209/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 210/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 211/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 212/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 213/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 214/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 215/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 216/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 217/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 218/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 219/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 220/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 221/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 222/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 223/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 224/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 225/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 226/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 227/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 228/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 229/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 230/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 231/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 232/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 233/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 234/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 235/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 236/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 237/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 238/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 239/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 240/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 241/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 242/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 243/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 244/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 245/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 246/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 247/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 248/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 249/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 250/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 251/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 252/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 253/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 254/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 255/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 256/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 257/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 258/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 259/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 260/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 261/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 262/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 263/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 264/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 265/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 266/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 267/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 268/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 269/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 270/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 271/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 272/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 273/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 274/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 275/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 276/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 277/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 278/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 279/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 280/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 281/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 282/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 283/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 284/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 285/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 286/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 287/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 288/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 289/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 290/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 291/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 292/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 293/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 294/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 295/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 296/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 297/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 298/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 299/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 300/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 301/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 302/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 303/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 304/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 305/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 306/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 307/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 308/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 309/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 310/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 311/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 312/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 313/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 314/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 315/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 316/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 317/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 318/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 319/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 320/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 321/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 322/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 323/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 324/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 325/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 326/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 327/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 328/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 329/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 330/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 331/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 332/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 333/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 334/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 335/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 336/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 337/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 338/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 339/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 340/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 341/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 342/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 343/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 344/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 345/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 346/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 347/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 348/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 349/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 350/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 351/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 352/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 353/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 354/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 355/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 356/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 357/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 358/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 359/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 360/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 361/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 362/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 363/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 364/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 365/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 366/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 367/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 368/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 369/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 370/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 371/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 372/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 373/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 374/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 375/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 376/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 377/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 378/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 379/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 380/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 381/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 382/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 383/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 384/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 385/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 386/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 387/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 388/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 389/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 390/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 391/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 392/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 393/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 394/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 395/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 396/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 397/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 398/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 399/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 400/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 401/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 402/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 403/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 404/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 405/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 406/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 407/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 408/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 409/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 410/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 411/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 412/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 413/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 414/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 415/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 416/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 417/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 418/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 419/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 420/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 421/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 422/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 423/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 424/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 425/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 426/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 427/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 428/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 429/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 430/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 431/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 432/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 433/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 434/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 435/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 436/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 437/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 438/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 439/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 440/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 441/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 442/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 443/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 444/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 445/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 446/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 447/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 448/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 449/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 450/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 451/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 452/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 453/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 454/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 455/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 456/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 457/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 458/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 459/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 460/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 461/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 462/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 463/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 464/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 465/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 466/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 467/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 468/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 469/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 470/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 471/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 472/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 473/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 474/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 475/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 476/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 477/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 478/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 479/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 480/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 481/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 482/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 483/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 484/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 485/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 486/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 487/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 488/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 489/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 490/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 491/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 492/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 493/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 494/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 495/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 496/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 497/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 498/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 499/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n",
            "Epoch 500/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145179455488.0000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 1b\n",
        "\n",
        "# Separate the features and the target variable\n",
        "features = housing_data.drop('price', axis=1)\n",
        "target = housing_data['price']\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_columns = features.select_dtypes(include=['object']).columns\n",
        "numerical_columns = features.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Define preprocessor with scaling for numerical and encoding for categorical data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_columns),\n",
        "        ('cat', OneHotEncoder(), categorical_columns)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply preprocessing to the features\n",
        "X_processed = preprocessor.fit_transform(features)\n",
        "\n",
        "# Convert the processed features and target to PyTorch tensors\n",
        "X_tensor = torch.tensor(X_processed.astype(np.float32))\n",
        "y_tensor = torch.tensor(target.values.astype(np.float32)).unsqueeze(1)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a neural network model with three hidden layers\n",
        "input_size = X_train.shape[1]\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(input_size, 32),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(32, 64),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(64, 16),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(16, 1)  # No activation function at the output for regression\n",
        ")\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "epochs = 500\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    y_train_pred = model(X_train)\n",
        "    train_loss = criterion(y_train_pred, y_train)\n",
        "    train_losses.append(train_loss.item())\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights\n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validation loss\n",
        "    with torch.no_grad():\n",
        "        y_val_pred = model(X_val)\n",
        "        val_loss = criterion(y_val_pred, y_val)\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "    # Print the training and validation loss with the epoch number\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Training Loss: {train_loss:.4f} - Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "# Plotting training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss - 3 hidden layers')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hw68hW8JK5fS",
        "outputId": "30a2e4aa-8ba9-48b9-8a47-cf5d6077d6d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500 - Training Loss: 25234788843520.0000 - Validation Loss: 30091037900800.0000\n",
            "Epoch 2/500 - Training Loss: 25198178861056.0000 - Validation Loss: 28516540219392.0000\n",
            "Epoch 3/500 - Training Loss: 23719936262144.0000 - Validation Loss: 27044125605888.0000\n",
            "Epoch 4/500 - Training Loss: 22340507271168.0000 - Validation Loss: 25667081076736.0000\n",
            "Epoch 5/500 - Training Loss: 21053283762176.0000 - Validation Loss: 24379129856000.0000\n",
            "Epoch 6/500 - Training Loss: 19852104302592.0000 - Validation Loss: 23174427181056.0000\n",
            "Epoch 7/500 - Training Loss: 18731218501632.0000 - Validation Loss: 22047497388032.0000\n",
            "Epoch 8/500 - Training Loss: 17685255553024.0000 - Validation Loss: 20993233911808.0000\n",
            "Epoch 9/500 - Training Loss: 16709208506368.0000 - Validation Loss: 20006872023040.0000\n",
            "Epoch 10/500 - Training Loss: 15798402809856.0000 - Validation Loss: 19083961565184.0000\n",
            "Epoch 11/500 - Training Loss: 14948480581632.0000 - Validation Loss: 18220348080128.0000\n",
            "Epoch 12/500 - Training Loss: 14155372298240.0000 - Validation Loss: 17412142399488.0000\n",
            "Epoch 13/500 - Training Loss: 13415277920256.0000 - Validation Loss: 16655725887488.0000\n",
            "Epoch 14/500 - Training Loss: 12724652212224.0000 - Validation Loss: 15947710595072.0000\n",
            "Epoch 15/500 - Training Loss: 12080190062592.0000 - Validation Loss: 15284931919872.0000\n",
            "Epoch 16/500 - Training Loss: 11478808657920.0000 - Validation Loss: 14664440217600.0000\n",
            "Epoch 17/500 - Training Loss: 10917625462784.0000 - Validation Loss: 14083480879104.0000\n",
            "Epoch 18/500 - Training Loss: 10393955074048.0000 - Validation Loss: 13539469164544.0000\n",
            "Epoch 19/500 - Training Loss: 9905288249344.0000 - Validation Loss: 13030003834880.0000\n",
            "Epoch 20/500 - Training Loss: 9449283518464.0000 - Validation Loss: 12552839888896.0000\n",
            "Epoch 21/500 - Training Loss: 9023764037632.0000 - Validation Loss: 12105873883136.0000\n",
            "Epoch 22/500 - Training Loss: 8626685083648.0000 - Validation Loss: 11687144980480.0000\n",
            "Epoch 23/500 - Training Loss: 8256148209664.0000 - Validation Loss: 11294825512960.0000\n",
            "Epoch 24/500 - Training Loss: 7910380797952.0000 - Validation Loss: 10927200010240.0000\n",
            "Epoch 25/500 - Training Loss: 7587726098432.0000 - Validation Loss: 10582670442496.0000\n",
            "Epoch 26/500 - Training Loss: 7286637985792.0000 - Validation Loss: 10259743637504.0000\n",
            "Epoch 27/500 - Training Loss: 7005677289472.0000 - Validation Loss: 9957024989184.0000\n",
            "Epoch 28/500 - Training Loss: 6743495540736.0000 - Validation Loss: 9673211117568.0000\n",
            "Epoch 29/500 - Training Loss: 6498839691264.0000 - Validation Loss: 9407079383040.0000\n",
            "Epoch 30/500 - Training Loss: 6270536908800.0000 - Validation Loss: 9157497323520.0000\n",
            "Epoch 31/500 - Training Loss: 6057496150016.0000 - Validation Loss: 8923398537216.0000\n",
            "Epoch 32/500 - Training Loss: 5858695577600.0000 - Validation Loss: 8703784255488.0000\n",
            "Epoch 33/500 - Training Loss: 5673182560256.0000 - Validation Loss: 8497733828608.0000\n",
            "Epoch 34/500 - Training Loss: 5500071575552.0000 - Validation Loss: 8304372744192.0000\n",
            "Epoch 35/500 - Training Loss: 5338530054144.0000 - Validation Loss: 8122893074432.0000\n",
            "Epoch 36/500 - Training Loss: 5187788341248.0000 - Validation Loss: 7952533553152.0000\n",
            "Epoch 37/500 - Training Loss: 5047121346560.0000 - Validation Loss: 7792588488704.0000\n",
            "Epoch 38/500 - Training Loss: 4915858505728.0000 - Validation Loss: 7642388889600.0000\n",
            "Epoch 39/500 - Training Loss: 4793368576000.0000 - Validation Loss: 7501322387456.0000\n",
            "Epoch 40/500 - Training Loss: 4679066976256.0000 - Validation Loss: 7368803876864.0000\n",
            "Epoch 41/500 - Training Loss: 4572405825536.0000 - Validation Loss: 7244294914048.0000\n",
            "Epoch 42/500 - Training Loss: 4472873943040.0000 - Validation Loss: 7127289561088.0000\n",
            "Epoch 43/500 - Training Loss: 4379994750976.0000 - Validation Loss: 7017311764480.0000\n",
            "Epoch 44/500 - Training Loss: 4293324963840.0000 - Validation Loss: 6913919549440.0000\n",
            "Epoch 45/500 - Training Loss: 4212448034816.0000 - Validation Loss: 6816699777024.0000\n",
            "Epoch 46/500 - Training Loss: 4136977563648.0000 - Validation Loss: 6725263949824.0000\n",
            "Epoch 47/500 - Training Loss: 4066551791616.0000 - Validation Loss: 6639249784832.0000\n",
            "Epoch 48/500 - Training Loss: 4000832815104.0000 - Validation Loss: 6558319640576.0000\n",
            "Epoch 49/500 - Training Loss: 3939507896320.0000 - Validation Loss: 6482154749952.0000\n",
            "Epoch 50/500 - Training Loss: 3882281861120.0000 - Validation Loss: 6410457317376.0000\n",
            "Epoch 51/500 - Training Loss: 3828880506880.0000 - Validation Loss: 6342953664512.0000\n",
            "Epoch 52/500 - Training Loss: 3779049553920.0000 - Validation Loss: 6279382171648.0000\n",
            "Epoch 53/500 - Training Loss: 3732548878336.0000 - Validation Loss: 6219497472000.0000\n",
            "Epoch 54/500 - Training Loss: 3689156444160.0000 - Validation Loss: 6163074121728.0000\n",
            "Epoch 55/500 - Training Loss: 3648664371200.0000 - Validation Loss: 6109900832768.0000\n",
            "Epoch 56/500 - Training Loss: 3610880245760.0000 - Validation Loss: 6059775229952.0000\n",
            "Epoch 57/500 - Training Loss: 3575620304896.0000 - Validation Loss: 6012512239616.0000\n",
            "Epoch 58/500 - Training Loss: 3542718611456.0000 - Validation Loss: 5967937273856.0000\n",
            "Epoch 59/500 - Training Loss: 3512015519744.0000 - Validation Loss: 5925885181952.0000\n",
            "Epoch 60/500 - Training Loss: 3483363966976.0000 - Validation Loss: 5886203920384.0000\n",
            "Epoch 61/500 - Training Loss: 3456628686848.0000 - Validation Loss: 5848750882816.0000\n",
            "Epoch 62/500 - Training Loss: 3431679393792.0000 - Validation Loss: 5813390802944.0000\n",
            "Epoch 63/500 - Training Loss: 3408398909440.0000 - Validation Loss: 5779997327360.0000\n",
            "Epoch 64/500 - Training Loss: 3386674249728.0000 - Validation Loss: 5748452491264.0000\n",
            "Epoch 65/500 - Training Loss: 3366401867776.0000 - Validation Loss: 5718645669888.0000\n",
            "Epoch 66/500 - Training Loss: 3347484246016.0000 - Validation Loss: 5690474102784.0000\n",
            "Epoch 67/500 - Training Loss: 3329831731200.0000 - Validation Loss: 5663839223808.0000\n",
            "Epoch 68/500 - Training Loss: 3313358602240.0000 - Validation Loss: 5638651904000.0000\n",
            "Epoch 69/500 - Training Loss: 3297986740224.0000 - Validation Loss: 5614826160128.0000\n",
            "Epoch 70/500 - Training Loss: 3283642220544.0000 - Validation Loss: 5592280203264.0000\n",
            "Epoch 71/500 - Training Loss: 3270256623616.0000 - Validation Loss: 5570940633088.0000\n",
            "Epoch 72/500 - Training Loss: 3257765724160.0000 - Validation Loss: 5550739292160.0000\n",
            "Epoch 73/500 - Training Loss: 3246110277632.0000 - Validation Loss: 5531604877312.0000\n",
            "Epoch 74/500 - Training Loss: 3235233398784.0000 - Validation Loss: 5513479716864.0000\n",
            "Epoch 75/500 - Training Loss: 3225083969536.0000 - Validation Loss: 5496304041984.0000\n",
            "Epoch 76/500 - Training Loss: 3215612706816.0000 - Validation Loss: 5480022278144.0000\n",
            "Epoch 77/500 - Training Loss: 3206774259712.0000 - Validation Loss: 5464585666560.0000\n",
            "Epoch 78/500 - Training Loss: 3198527209472.0000 - Validation Loss: 5449943875584.0000\n",
            "Epoch 79/500 - Training Loss: 3190831185920.0000 - Validation Loss: 5436053389312.0000\n",
            "Epoch 80/500 - Training Loss: 3183649226752.0000 - Validation Loss: 5422870167552.0000\n",
            "Epoch 81/500 - Training Loss: 3176947777536.0000 - Validation Loss: 5410356461568.0000\n",
            "Epoch 82/500 - Training Loss: 3170694070272.0000 - Validation Loss: 5398473474048.0000\n",
            "Epoch 83/500 - Training Loss: 3164858744832.0000 - Validation Loss: 5387185553408.0000\n",
            "Epoch 84/500 - Training Loss: 3159412965376.0000 - Validation Loss: 5376460718080.0000\n",
            "Epoch 85/500 - Training Loss: 3154331828224.0000 - Validation Loss: 5366266986496.0000\n",
            "Epoch 86/500 - Training Loss: 3149589643264.0000 - Validation Loss: 5356575522816.0000\n",
            "Epoch 87/500 - Training Loss: 3145164914688.0000 - Validation Loss: 5347359064064.0000\n",
            "Epoch 88/500 - Training Loss: 3141035884544.0000 - Validation Loss: 5338591920128.0000\n",
            "Epoch 89/500 - Training Loss: 3137182629888.0000 - Validation Loss: 5330248925184.0000\n",
            "Epoch 90/500 - Training Loss: 3133587062784.0000 - Validation Loss: 5322308059136.0000\n",
            "Epoch 91/500 - Training Loss: 3130231881728.0000 - Validation Loss: 5314746777600.0000\n",
            "Epoch 92/500 - Training Loss: 3127100571648.0000 - Validation Loss: 5307545681920.0000\n",
            "Epoch 93/500 - Training Loss: 3124179238912.0000 - Validation Loss: 5300685897728.0000\n",
            "Epoch 94/500 - Training Loss: 3121452941312.0000 - Validation Loss: 5294149074944.0000\n",
            "Epoch 95/500 - Training Loss: 3118909095936.0000 - Validation Loss: 5287916863488.0000\n",
            "Epoch 96/500 - Training Loss: 3116534595584.0000 - Validation Loss: 5281975631872.0000\n",
            "Epoch 97/500 - Training Loss: 3114319478784.0000 - Validation Loss: 5276308078592.0000\n",
            "Epoch 98/500 - Training Loss: 3112251949056.0000 - Validation Loss: 5270902669312.0000\n",
            "Epoch 99/500 - Training Loss: 3110323355648.0000 - Validation Loss: 5265743151104.0000\n",
            "Epoch 100/500 - Training Loss: 3108523212800.0000 - Validation Loss: 5260818513920.0000\n",
            "Epoch 101/500 - Training Loss: 3106843131904.0000 - Validation Loss: 5256115650560.0000\n",
            "Epoch 102/500 - Training Loss: 3105275248640.0000 - Validation Loss: 5251626172416.0000\n",
            "Epoch 103/500 - Training Loss: 3103813009408.0000 - Validation Loss: 5247335399424.0000\n",
            "Epoch 104/500 - Training Loss: 3102448025600.0000 - Validation Loss: 5243236515840.0000\n",
            "Epoch 105/500 - Training Loss: 3101173743616.0000 - Validation Loss: 5239318511616.0000\n",
            "Epoch 106/500 - Training Loss: 3099985182720.0000 - Validation Loss: 5235571949568.0000\n",
            "Epoch 107/500 - Training Loss: 3098876575744.0000 - Validation Loss: 5231990538240.0000\n",
            "Epoch 108/500 - Training Loss: 3097841106944.0000 - Validation Loss: 5228564316160.0000\n",
            "Epoch 109/500 - Training Loss: 3096875892736.0000 - Validation Loss: 5225285943296.0000\n",
            "Epoch 110/500 - Training Loss: 3095973855232.0000 - Validation Loss: 5222150176768.0000\n",
            "Epoch 111/500 - Training Loss: 3095133159424.0000 - Validation Loss: 5219146006528.0000\n",
            "Epoch 112/500 - Training Loss: 3094348562432.0000 - Validation Loss: 5216271859712.0000\n",
            "Epoch 113/500 - Training Loss: 3093616132096.0000 - Validation Loss: 5213518823424.0000\n",
            "Epoch 114/500 - Training Loss: 3092932984832.0000 - Validation Loss: 5210882179072.0000\n",
            "Epoch 115/500 - Training Loss: 3092294926336.0000 - Validation Loss: 5208355635200.0000\n",
            "Epoch 116/500 - Training Loss: 3091699859456.0000 - Validation Loss: 5205934473216.0000\n",
            "Epoch 117/500 - Training Loss: 3091144376320.0000 - Validation Loss: 5203613974528.0000\n",
            "Epoch 118/500 - Training Loss: 3090626379776.0000 - Validation Loss: 5201389944832.0000\n",
            "Epoch 119/500 - Training Loss: 3090142724096.0000 - Validation Loss: 5199257141248.0000\n",
            "Epoch 120/500 - Training Loss: 3089691574272.0000 - Validation Loss: 5197211369472.0000\n",
            "Epoch 121/500 - Training Loss: 3089270308864.0000 - Validation Loss: 5195248959488.0000\n",
            "Epoch 122/500 - Training Loss: 3088877617152.0000 - Validation Loss: 5193365716992.0000\n",
            "Epoch 123/500 - Training Loss: 3088510877696.0000 - Validation Loss: 5191559544832.0000\n",
            "Epoch 124/500 - Training Loss: 3088169041920.0000 - Validation Loss: 5189825200128.0000\n",
            "Epoch 125/500 - Training Loss: 3087849488384.0000 - Validation Loss: 5188161110016.0000\n",
            "Epoch 126/500 - Training Loss: 3087551692800.0000 - Validation Loss: 5186563604480.0000\n",
            "Epoch 127/500 - Training Loss: 3087273295872.0000 - Validation Loss: 5185028489216.0000\n",
            "Epoch 128/500 - Training Loss: 3087014297600.0000 - Validation Loss: 5183555239936.0000\n",
            "Epoch 129/500 - Training Loss: 3086771814400.0000 - Validation Loss: 5182138089472.0000\n",
            "Epoch 130/500 - Training Loss: 3086546108416.0000 - Validation Loss: 5180778610688.0000\n",
            "Epoch 131/500 - Training Loss: 3086335344640.0000 - Validation Loss: 5179470512128.0000\n",
            "Epoch 132/500 - Training Loss: 3086138736640.0000 - Validation Loss: 5178214842368.0000\n",
            "Epoch 133/500 - Training Loss: 3085954973696.0000 - Validation Loss: 5177007407104.0000\n",
            "Epoch 134/500 - Training Loss: 3085783793664.0000 - Validation Loss: 5175846633472.0000\n",
            "Epoch 135/500 - Training Loss: 3085623623680.0000 - Validation Loss: 5174731472896.0000\n",
            "Epoch 136/500 - Training Loss: 3085474725888.0000 - Validation Loss: 5173656682496.0000\n",
            "Epoch 137/500 - Training Loss: 3085335527424.0000 - Validation Loss: 5172625408000.0000\n",
            "Epoch 138/500 - Training Loss: 3085205766144.0000 - Validation Loss: 5171631882240.0000\n",
            "Epoch 139/500 - Training Loss: 3085084131328.0000 - Validation Loss: 5170677678080.0000\n",
            "Epoch 140/500 - Training Loss: 3084971409408.0000 - Validation Loss: 5169758601216.0000\n",
            "Epoch 141/500 - Training Loss: 3084865503232.0000 - Validation Loss: 5168874127360.0000\n",
            "Epoch 142/500 - Training Loss: 3084767199232.0000 - Validation Loss: 5168022683648.0000\n",
            "Epoch 143/500 - Training Loss: 3084675448832.0000 - Validation Loss: 5167204794368.0000\n",
            "Epoch 144/500 - Training Loss: 3084589989888.0000 - Validation Loss: 5166416789504.0000\n",
            "Epoch 145/500 - Training Loss: 3084509249536.0000 - Validation Loss: 5165658144768.0000\n",
            "Epoch 146/500 - Training Loss: 3084434800640.0000 - Validation Loss: 5164927811584.0000\n",
            "Epoch 147/500 - Training Loss: 3084365332480.0000 - Validation Loss: 5164224217088.0000\n",
            "Epoch 148/500 - Training Loss: 3084300320768.0000 - Validation Loss: 5163546312704.0000\n",
            "Epoch 149/500 - Training Loss: 3084238979072.0000 - Validation Loss: 5162894098432.0000\n",
            "Epoch 150/500 - Training Loss: 3084183142400.0000 - Validation Loss: 5162266001408.0000\n",
            "Epoch 151/500 - Training Loss: 3084130451456.0000 - Validation Loss: 5161660973056.0000\n",
            "Epoch 152/500 - Training Loss: 3084080906240.0000 - Validation Loss: 5161077440512.0000\n",
            "Epoch 153/500 - Training Loss: 3084035031040.0000 - Validation Loss: 5160515403776.0000\n",
            "Epoch 154/500 - Training Loss: 3083991777280.0000 - Validation Loss: 5159974338560.0000\n",
            "Epoch 155/500 - Training Loss: 3083951407104.0000 - Validation Loss: 5159453720576.0000\n",
            "Epoch 156/500 - Training Loss: 3083914706944.0000 - Validation Loss: 5158950928384.0000\n",
            "Epoch 157/500 - Training Loss: 3083879579648.0000 - Validation Loss: 5158465961984.0000\n",
            "Epoch 158/500 - Training Loss: 3083846811648.0000 - Validation Loss: 5157999869952.0000\n",
            "Epoch 159/500 - Training Loss: 3083816665088.0000 - Validation Loss: 5157549506560.0000\n",
            "Epoch 160/500 - Training Loss: 3083788353536.0000 - Validation Loss: 5157116968960.0000\n",
            "Epoch 161/500 - Training Loss: 3083761614848.0000 - Validation Loss: 5156698587136.0000\n",
            "Epoch 162/500 - Training Loss: 3083736973312.0000 - Validation Loss: 5156294885376.0000\n",
            "Epoch 163/500 - Training Loss: 3083713904640.0000 - Validation Loss: 5155906912256.0000\n",
            "Epoch 164/500 - Training Loss: 3083692408832.0000 - Validation Loss: 5155532046336.0000\n",
            "Epoch 165/500 - Training Loss: 3083672748032.0000 - Validation Loss: 5155170811904.0000\n",
            "Epoch 166/500 - Training Loss: 3083653611520.0000 - Validation Loss: 5154822160384.0000\n",
            "Epoch 167/500 - Training Loss: 3083636310016.0000 - Validation Loss: 5154486616064.0000\n",
            "Epoch 168/500 - Training Loss: 3083620057088.0000 - Validation Loss: 5154162606080.0000\n",
            "Epoch 169/500 - Training Loss: 3083604852736.0000 - Validation Loss: 5153850130432.0000\n",
            "Epoch 170/500 - Training Loss: 3083590434816.0000 - Validation Loss: 5153548140544.0000\n",
            "Epoch 171/500 - Training Loss: 3083577065472.0000 - Validation Loss: 5153257684992.0000\n",
            "Epoch 172/500 - Training Loss: 3083565268992.0000 - Validation Loss: 5152977715200.0000\n",
            "Epoch 173/500 - Training Loss: 3083553472512.0000 - Validation Loss: 5152706658304.0000\n",
            "Epoch 174/500 - Training Loss: 3083542724608.0000 - Validation Loss: 5152445038592.0000\n",
            "Epoch 175/500 - Training Loss: 3083532238848.0000 - Validation Loss: 5152193380352.0000\n",
            "Epoch 176/500 - Training Loss: 3083523063808.0000 - Validation Loss: 5151950635008.0000\n",
            "Epoch 177/500 - Training Loss: 3083514413056.0000 - Validation Loss: 5151716802560.0000\n",
            "Epoch 178/500 - Training Loss: 3083506286592.0000 - Validation Loss: 5151490310144.0000\n",
            "Epoch 179/500 - Training Loss: 3083498684416.0000 - Validation Loss: 5151272730624.0000\n",
            "Epoch 180/500 - Training Loss: 3083491606528.0000 - Validation Loss: 5151061442560.0000\n",
            "Epoch 181/500 - Training Loss: 3083485052928.0000 - Validation Loss: 5150859067392.0000\n",
            "Epoch 182/500 - Training Loss: 3083478761472.0000 - Validation Loss: 5150662459392.0000\n",
            "Epoch 183/500 - Training Loss: 3083473256448.0000 - Validation Loss: 5150473715712.0000\n",
            "Epoch 184/500 - Training Loss: 3083467489280.0000 - Validation Loss: 5150291263488.0000\n",
            "Epoch 185/500 - Training Loss: 3083462246400.0000 - Validation Loss: 5150115102720.0000\n",
            "Epoch 186/500 - Training Loss: 3083457789952.0000 - Validation Loss: 5149945233408.0000\n",
            "Epoch 187/500 - Training Loss: 3083453333504.0000 - Validation Loss: 5149781131264.0000\n",
            "Epoch 188/500 - Training Loss: 3083449663488.0000 - Validation Loss: 5149622796288.0000\n",
            "Epoch 189/500 - Training Loss: 3083445731328.0000 - Validation Loss: 5149469704192.0000\n",
            "Epoch 190/500 - Training Loss: 3083442323456.0000 - Validation Loss: 5149322379264.0000\n",
            "Epoch 191/500 - Training Loss: 3083438653440.0000 - Validation Loss: 5149179772928.0000\n",
            "Epoch 192/500 - Training Loss: 3083435507712.0000 - Validation Loss: 5149041885184.0000\n",
            "Epoch 193/500 - Training Loss: 3083433148416.0000 - Validation Loss: 5148909240320.0000\n",
            "Epoch 194/500 - Training Loss: 3083430002688.0000 - Validation Loss: 5148780789760.0000\n",
            "Epoch 195/500 - Training Loss: 3083427643392.0000 - Validation Loss: 5148657582080.0000\n",
            "Epoch 196/500 - Training Loss: 3083425546240.0000 - Validation Loss: 5148536995840.0000\n",
            "Epoch 197/500 - Training Loss: 3083422924800.0000 - Validation Loss: 5148422176768.0000\n",
            "Epoch 198/500 - Training Loss: 3083421089792.0000 - Validation Loss: 5148311027712.0000\n",
            "Epoch 199/500 - Training Loss: 3083418992640.0000 - Validation Loss: 5148204072960.0000\n",
            "Epoch 200/500 - Training Loss: 3083417419776.0000 - Validation Loss: 5148100263936.0000\n",
            "Epoch 201/500 - Training Loss: 3083415584768.0000 - Validation Loss: 5148000124928.0000\n",
            "Epoch 202/500 - Training Loss: 3083414274048.0000 - Validation Loss: 5147903655936.0000\n",
            "Epoch 203/500 - Training Loss: 3083412701184.0000 - Validation Loss: 5147810332672.0000\n",
            "Epoch 204/500 - Training Loss: 3083411390464.0000 - Validation Loss: 5147720155136.0000\n",
            "Epoch 205/500 - Training Loss: 3083410079744.0000 - Validation Loss: 5147632074752.0000\n",
            "Epoch 206/500 - Training Loss: 3083408769024.0000 - Validation Loss: 5147549237248.0000\n",
            "Epoch 207/500 - Training Loss: 3083407458304.0000 - Validation Loss: 5147466924032.0000\n",
            "Epoch 208/500 - Training Loss: 3083406671872.0000 - Validation Loss: 5147388805120.0000\n",
            "Epoch 209/500 - Training Loss: 3083406147584.0000 - Validation Loss: 5147312783360.0000\n",
            "Epoch 210/500 - Training Loss: 3083404836864.0000 - Validation Loss: 5147239907328.0000\n",
            "Epoch 211/500 - Training Loss: 3083403788288.0000 - Validation Loss: 5147169652736.0000\n",
            "Epoch 212/500 - Training Loss: 3083403264000.0000 - Validation Loss: 5147102019584.0000\n",
            "Epoch 213/500 - Training Loss: 3083402739712.0000 - Validation Loss: 5147035435008.0000\n",
            "Epoch 214/500 - Training Loss: 3083401691136.0000 - Validation Loss: 5146971996160.0000\n",
            "Epoch 215/500 - Training Loss: 3083400904704.0000 - Validation Loss: 5146910654464.0000\n",
            "Epoch 216/500 - Training Loss: 3083400904704.0000 - Validation Loss: 5146851934208.0000\n",
            "Epoch 217/500 - Training Loss: 3083400118272.0000 - Validation Loss: 5146794262528.0000\n",
            "Epoch 218/500 - Training Loss: 3083399593984.0000 - Validation Loss: 5146739212288.0000\n",
            "Epoch 219/500 - Training Loss: 3083399331840.0000 - Validation Loss: 5146685734912.0000\n",
            "Epoch 220/500 - Training Loss: 3083398545408.0000 - Validation Loss: 5146634354688.0000\n",
            "Epoch 221/500 - Training Loss: 3083398021120.0000 - Validation Loss: 5146584547328.0000\n",
            "Epoch 222/500 - Training Loss: 3083398021120.0000 - Validation Loss: 5146536837120.0000\n",
            "Epoch 223/500 - Training Loss: 3083397496832.0000 - Validation Loss: 5146490175488.0000\n",
            "Epoch 224/500 - Training Loss: 3083396710400.0000 - Validation Loss: 5146445611008.0000\n",
            "Epoch 225/500 - Training Loss: 3083396710400.0000 - Validation Loss: 5146402619392.0000\n",
            "Epoch 226/500 - Training Loss: 3083396448256.0000 - Validation Loss: 5146360676352.0000\n",
            "Epoch 227/500 - Training Loss: 3083396186112.0000 - Validation Loss: 5146320306176.0000\n",
            "Epoch 228/500 - Training Loss: 3083395923968.0000 - Validation Loss: 5146280984576.0000\n",
            "Epoch 229/500 - Training Loss: 3083395661824.0000 - Validation Loss: 5146243760128.0000\n",
            "Epoch 230/500 - Training Loss: 3083395661824.0000 - Validation Loss: 5146207584256.0000\n",
            "Epoch 231/500 - Training Loss: 3083395661824.0000 - Validation Loss: 5146171932672.0000\n",
            "Epoch 232/500 - Training Loss: 3083394875392.0000 - Validation Loss: 5146138378240.0000\n",
            "Epoch 233/500 - Training Loss: 3083394875392.0000 - Validation Loss: 5146105348096.0000\n",
            "Epoch 234/500 - Training Loss: 3083394875392.0000 - Validation Loss: 5146074415104.0000\n",
            "Epoch 235/500 - Training Loss: 3083394613248.0000 - Validation Loss: 5146043482112.0000\n",
            "Epoch 236/500 - Training Loss: 3083394351104.0000 - Validation Loss: 5146014121984.0000\n",
            "Epoch 237/500 - Training Loss: 3083394351104.0000 - Validation Loss: 5145985810432.0000\n",
            "Epoch 238/500 - Training Loss: 3083394088960.0000 - Validation Loss: 5145958023168.0000\n",
            "Epoch 239/500 - Training Loss: 3083394088960.0000 - Validation Loss: 5145930760192.0000\n",
            "Epoch 240/500 - Training Loss: 3083394088960.0000 - Validation Loss: 5145905070080.0000\n",
            "Epoch 241/500 - Training Loss: 3083394088960.0000 - Validation Loss: 5145880952832.0000\n",
            "Epoch 242/500 - Training Loss: 3083393826816.0000 - Validation Loss: 5145856835584.0000\n",
            "Epoch 243/500 - Training Loss: 3083393302528.0000 - Validation Loss: 5145833242624.0000\n",
            "Epoch 244/500 - Training Loss: 3083393826816.0000 - Validation Loss: 5145810698240.0000\n",
            "Epoch 245/500 - Training Loss: 3083393826816.0000 - Validation Loss: 5145789726720.0000\n",
            "Epoch 246/500 - Training Loss: 3083393302528.0000 - Validation Loss: 5145768755200.0000\n",
            "Epoch 247/500 - Training Loss: 3083393302528.0000 - Validation Loss: 5145749356544.0000\n",
            "Epoch 248/500 - Training Loss: 3083393040384.0000 - Validation Loss: 5145728909312.0000\n",
            "Epoch 249/500 - Training Loss: 3083393040384.0000 - Validation Loss: 5145710034944.0000\n",
            "Epoch 250/500 - Training Loss: 3083392778240.0000 - Validation Loss: 5145691160576.0000\n",
            "Epoch 251/500 - Training Loss: 3083393040384.0000 - Validation Loss: 5145674383360.0000\n",
            "Epoch 252/500 - Training Loss: 3083392778240.0000 - Validation Loss: 5145656557568.0000\n",
            "Epoch 253/500 - Training Loss: 3083392778240.0000 - Validation Loss: 5145641353216.0000\n",
            "Epoch 254/500 - Training Loss: 3083392778240.0000 - Validation Loss: 5145625624576.0000\n",
            "Epoch 255/500 - Training Loss: 3083392778240.0000 - Validation Loss: 5145608847360.0000\n",
            "Epoch 256/500 - Training Loss: 3083393040384.0000 - Validation Loss: 5145594691584.0000\n",
            "Epoch 257/500 - Training Loss: 3083393040384.0000 - Validation Loss: 5145580535808.0000\n",
            "Epoch 258/500 - Training Loss: 3083393040384.0000 - Validation Loss: 5145567428608.0000\n",
            "Epoch 259/500 - Training Loss: 3083392778240.0000 - Validation Loss: 5145553797120.0000\n",
            "Epoch 260/500 - Training Loss: 3083392778240.0000 - Validation Loss: 5145540689920.0000\n",
            "Epoch 261/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145528631296.0000\n",
            "Epoch 262/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145516048384.0000\n",
            "Epoch 263/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145505038336.0000\n",
            "Epoch 264/500 - Training Loss: 3083392778240.0000 - Validation Loss: 5145494028288.0000\n",
            "Epoch 265/500 - Training Loss: 3083392778240.0000 - Validation Loss: 5145483018240.0000\n",
            "Epoch 266/500 - Training Loss: 3083392778240.0000 - Validation Loss: 5145472532480.0000\n",
            "Epoch 267/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145462571008.0000\n",
            "Epoch 268/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145452609536.0000\n",
            "Epoch 269/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145443172352.0000\n",
            "Epoch 270/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145434259456.0000\n",
            "Epoch 271/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145425870848.0000\n",
            "Epoch 272/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145417482240.0000\n",
            "Epoch 273/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145409093632.0000\n",
            "Epoch 274/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145401229312.0000\n",
            "Epoch 275/500 - Training Loss: 3083392778240.0000 - Validation Loss: 5145393364992.0000\n",
            "Epoch 276/500 - Training Loss: 3083392778240.0000 - Validation Loss: 5145386024960.0000\n",
            "Epoch 277/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145379209216.0000\n",
            "Epoch 278/500 - Training Loss: 3083392778240.0000 - Validation Loss: 5145371869184.0000\n",
            "Epoch 279/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145365053440.0000\n",
            "Epoch 280/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145358761984.0000\n",
            "Epoch 281/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145353519104.0000\n",
            "Epoch 282/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145346703360.0000\n",
            "Epoch 283/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145341460480.0000\n",
            "Epoch 284/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145335693312.0000\n",
            "Epoch 285/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145329926144.0000\n",
            "Epoch 286/500 - Training Loss: 3083392778240.0000 - Validation Loss: 5145324683264.0000\n",
            "Epoch 287/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145320488960.0000\n",
            "Epoch 288/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145315246080.0000\n",
            "Epoch 289/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145310527488.0000\n",
            "Epoch 290/500 - Training Loss: 3083392778240.0000 - Validation Loss: 5145305808896.0000\n",
            "Epoch 291/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145301090304.0000\n",
            "Epoch 292/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145297420288.0000\n",
            "Epoch 293/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145293750272.0000\n",
            "Epoch 294/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145288507392.0000\n",
            "Epoch 295/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145285361664.0000\n",
            "Epoch 296/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145281167360.0000\n",
            "Epoch 297/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145278545920.0000\n",
            "Epoch 298/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145274351616.0000\n",
            "Epoch 299/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145271205888.0000\n",
            "Epoch 300/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145268060160.0000\n",
            "Epoch 301/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145264914432.0000\n",
            "Epoch 302/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145262292992.0000\n",
            "Epoch 303/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145258622976.0000\n",
            "Epoch 304/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145256001536.0000\n",
            "Epoch 305/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145253904384.0000\n",
            "Epoch 306/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145251282944.0000\n",
            "Epoch 307/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145248661504.0000\n",
            "Epoch 308/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145246564352.0000\n",
            "Epoch 309/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145243942912.0000\n",
            "Epoch 310/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145241321472.0000\n",
            "Epoch 311/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145238700032.0000\n",
            "Epoch 312/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145237127168.0000\n",
            "Epoch 313/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145235554304.0000\n",
            "Epoch 314/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145233457152.0000\n",
            "Epoch 315/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145231884288.0000\n",
            "Epoch 316/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145229787136.0000\n",
            "Epoch 317/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145227165696.0000\n",
            "Epoch 318/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145225592832.0000\n",
            "Epoch 319/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145224019968.0000\n",
            "Epoch 320/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145222971392.0000\n",
            "Epoch 321/500 - Training Loss: 3083392778240.0000 - Validation Loss: 5145221398528.0000\n",
            "Epoch 322/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145219301376.0000\n",
            "Epoch 323/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145217728512.0000\n",
            "Epoch 324/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145216679936.0000\n",
            "Epoch 325/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145215107072.0000\n",
            "Epoch 326/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145214058496.0000\n",
            "Epoch 327/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145212485632.0000\n",
            "Epoch 328/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145211437056.0000\n",
            "Epoch 329/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145210388480.0000\n",
            "Epoch 330/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145209339904.0000\n",
            "Epoch 331/500 - Training Loss: 3083391991808.0000 - Validation Loss: 5145207767040.0000\n",
            "Epoch 332/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145207242752.0000\n",
            "Epoch 333/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145206194176.0000\n",
            "Epoch 334/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145205145600.0000\n",
            "Epoch 335/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145204097024.0000\n",
            "Epoch 336/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145203572736.0000\n",
            "Epoch 337/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145201999872.0000\n",
            "Epoch 338/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145200951296.0000\n",
            "Epoch 339/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145199902720.0000\n",
            "Epoch 340/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145199378432.0000\n",
            "Epoch 341/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145198329856.0000\n",
            "Epoch 342/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145197281280.0000\n",
            "Epoch 343/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145197805568.0000\n",
            "Epoch 344/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145196756992.0000\n",
            "Epoch 345/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145196232704.0000\n",
            "Epoch 346/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145195708416.0000\n",
            "Epoch 347/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145195708416.0000\n",
            "Epoch 348/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145194659840.0000\n",
            "Epoch 349/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145193611264.0000\n",
            "Epoch 350/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145193086976.0000\n",
            "Epoch 351/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145192038400.0000\n",
            "Epoch 352/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145191514112.0000\n",
            "Epoch 353/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145190989824.0000\n",
            "Epoch 354/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145190989824.0000\n",
            "Epoch 355/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145190989824.0000\n",
            "Epoch 356/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145189941248.0000\n",
            "Epoch 357/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145189941248.0000\n",
            "Epoch 358/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145189416960.0000\n",
            "Epoch 359/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145189416960.0000\n",
            "Epoch 360/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145188368384.0000\n",
            "Epoch 361/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145188368384.0000\n",
            "Epoch 362/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145187844096.0000\n",
            "Epoch 363/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145187844096.0000\n",
            "Epoch 364/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145187319808.0000\n",
            "Epoch 365/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145187319808.0000\n",
            "Epoch 366/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145186795520.0000\n",
            "Epoch 367/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145186795520.0000\n",
            "Epoch 368/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145186795520.0000\n",
            "Epoch 369/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145186795520.0000\n",
            "Epoch 370/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145185746944.0000\n",
            "Epoch 371/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145185746944.0000\n",
            "Epoch 372/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145184698368.0000\n",
            "Epoch 373/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145184698368.0000\n",
            "Epoch 374/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145184698368.0000\n",
            "Epoch 375/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145184174080.0000\n",
            "Epoch 376/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145183649792.0000\n",
            "Epoch 377/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145183649792.0000\n",
            "Epoch 378/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145182601216.0000\n",
            "Epoch 379/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145182601216.0000\n",
            "Epoch 380/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145182601216.0000\n",
            "Epoch 381/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145182601216.0000\n",
            "Epoch 382/500 - Training Loss: 3083392253952.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 383/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 384/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 385/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 386/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 387/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 388/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 389/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 390/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 391/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 392/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 393/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 394/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 395/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 396/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 397/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 398/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 399/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 400/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 401/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 402/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 403/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 404/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 405/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 406/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 407/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 408/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 409/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 410/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 411/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 412/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 413/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 414/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 415/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 416/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 417/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 418/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 419/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 420/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 421/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 422/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 423/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 424/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 425/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 426/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 427/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 428/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 429/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 430/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 431/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 432/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 433/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 434/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 435/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 436/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 437/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 438/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 439/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 440/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 441/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 442/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 443/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 444/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 445/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 446/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 447/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 448/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 449/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 450/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 451/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 452/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 453/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 454/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 455/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 456/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 457/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 458/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 459/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 460/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 461/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 462/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 463/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 464/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 465/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 466/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 467/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 468/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 469/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 470/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 471/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 472/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 473/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 474/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 475/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 476/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 477/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 478/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 479/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 480/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 481/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 482/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 483/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 484/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 485/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 486/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 487/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 488/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 489/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 490/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 491/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 492/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 493/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 494/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 495/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 496/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 497/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 498/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 499/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n",
            "Epoch 500/500 - Training Loss: 3083392516096.0000 - Validation Loss: 5145181028352.0000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 2a\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "# Transformations for the input data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n",
        "])\n",
        "\n",
        "# Load the training and test sets\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "# Class labels in CIFAR-10\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Neural network architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(3 * 32 * 32, 512)  # CIFAR-10 images are 32x32x3\n",
        "        self.fc2 = nn.Linear(512, 10)  # 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the network\n",
        "start_time = time.time()\n",
        "for epoch in range(2):  # loop over the dataset multiple times (you can increase the number of epochs)\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# Test the network on the test data\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
        "print(f\"Training time: {training_time}s\")\n",
        "\n",
        "# Save the trained model\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-FYA_MTLtJE",
        "outputId": "6fce4e14-c076-4c7d-dd98-6da4a2cd43a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 80153808.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "[1,  2000] loss: 1.843\n",
            "[1,  4000] loss: 1.669\n",
            "[1,  6000] loss: 1.643\n",
            "[1,  8000] loss: 1.602\n",
            "[1, 10000] loss: 1.589\n",
            "[1, 12000] loss: 1.564\n",
            "[2,  2000] loss: 1.472\n",
            "[2,  4000] loss: 1.494\n",
            "[2,  6000] loss: 1.473\n",
            "[2,  8000] loss: 1.479\n",
            "[2, 10000] loss: 1.476\n",
            "[2, 12000] loss: 1.492\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 47 %\n",
            "Training time: 281.07781291007996s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 2b\n",
        "\n",
        "# Transformations for the input data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load the training and test sets\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "# Neural network architecture with additional layers\n",
        "class ExtendedNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ExtendedNet, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(3 * 32 * 32, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "net = ExtendedNet()\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the network for 300 epochs\n",
        "start_time = time.time()\n",
        "epochs = 300\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# Evaluate the network\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy of the network on the 10000 test images: {accuracy}%')\n",
        "print(f\"Training time: {training_time}s\")\n",
        "\n",
        "# Save the trained model\n",
        "PATH = './extended_cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "draSiO5sV0m3",
        "outputId": "a95dde54-85fe-4c60-e25d-5b345ce5ce88"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[1,  2000] loss: 2.042\n",
            "[1,  4000] loss: 1.764\n",
            "[1,  6000] loss: 1.688\n",
            "[1,  8000] loss: 1.637\n",
            "[1, 10000] loss: 1.575\n",
            "[1, 12000] loss: 1.550\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f27c6b9580a4>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1292\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mauthkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0manswer_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m         \u001b[0mdeliver_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36manswer_challenge\u001b[0;34m(connection, authkey)\u001b[0m\n\u001b[1;32m    750\u001b[0m         raise ValueError(\n\u001b[1;32m    751\u001b[0m             \"Authkey must be bytes, not {0!s}\".format(type(authkey)))\n\u001b[0;32m--> 752\u001b[0;31m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# reject large message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHALLENGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCHALLENGE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'message = %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHALLENGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}